[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate in the department of Computer Science and Engineering at Seoul National University (SNU), advised by Prof. Byung-Gon Chun. I am a member of Software Platform Lab (SPL). Before joining SPL, I worked at Samsung Research from 2013 to 2016. I hold a BS and an MS in the Department of Electrical and Computer Engineering at SNU (MS advisor: Prof. Soo-Mook Moon). My research focuses on deep learning systems, distributed systems, and compilers.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://ejjeong.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a PhD candidate in the department of Computer Science and Engineering at Seoul National University (SNU), advised by Prof. Byung-Gon Chun. I am a member of Software Platform Lab (SPL). Before joining SPL, I worked at Samsung Research from 2013 to 2016. I hold a BS and an MS in the Department of Electrical and Computer Engineering at SNU (MS advisor: Prof. Soo-Mook Moon). My research focuses on deep learning systems, distributed systems, and compilers.","tags":null,"title":"Eunji Jeong","type":"author"},{"authors":["Ahnjae Shin","Dong-Jin Shin","Sungwoo Cho","Do Yoon Kim","**Eunji Jeong**","Gyeong-In Yu","Byung-Gon Chun"],"categories":null,"content":"","date":1571062529,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571062529,"objectID":"5315dee82d7fbe62768d7d2b54d1c502","permalink":"https://ejjeong.github.io/publication/hippo/","publishdate":"2019-10-14T23:15:29+09:00","relpermalink":"/publication/hippo/","section":"publication","summary":"","tags":[],"title":"Stage-based Hyper-parameter Optimization for Deep Learning","type":"publication"},{"authors":["Eunji Jeong","Sungwoo Cho","Gyeong-In Yu","Joo Seong Jeong","Dong-Jin Shin","Taebum Kim","Byung-Gon Chun"],"categories":null,"content":"","date":1565100929,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565100929,"objectID":"0168d323217e3b67e539a91afbc7e286","permalink":"https://ejjeong.github.io/publication/janus-osr19/","publishdate":"2019-08-06T23:15:29+09:00","relpermalink":"/publication/janus-osr19/","section":"publication","summary":"","tags":[],"title":"Speculative Symbolic Graph Execution of Imperative Deep Learning Programs","type":"publication"},{"authors":["Eunji Jeong","Sungwoo Cho","Gyeong-In Yu","Joo Seong Jeong","Dong-Jin Shin","Byung-Gon Chun"],"categories":null,"content":"","date":1553985372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553985372,"objectID":"d4410808c638afd3dca87922db071b0c","permalink":"https://ejjeong.github.io/publication/janus-sysml19/","publishdate":"2019-03-30T15:36:12-07:00","relpermalink":"/publication/janus-sysml19/","section":"publication","summary":"","tags":[],"title":"Demonstration of JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs","type":"publication"},{"authors":["Soojeong Kim","Gyeong-In Yu","Hojin Park","Sungwoo Cho","Eunji Jeong","Hyeonmin Ha","Sanha Lee","Joo Seong Jeong","Byung-Gon Chun"],"categories":null,"content":"","date":1553551494,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553551494,"objectID":"ba3261cf993e75011942483deb887e1c","permalink":"https://ejjeong.github.io/publication/parallax-eurosys19/","publishdate":"2019-03-25T15:04:54-07:00","relpermalink":"/publication/parallax-eurosys19/","section":"publication","summary":"","tags":[],"title":"Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks","type":"publication"},{"authors":["Eunji Jeong","Sungwoo Cho","Gyeong-In Yu","Joo Seong Jeong","Dong-Jin Shin","Byung-Gon Chun"],"categories":null,"content":"","date":1551298559,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551298559,"objectID":"b666c410c75b153a054c326c5895a59c","permalink":"https://ejjeong.github.io/publication/janus-nsdi19/","publishdate":"2019-02-27T13:15:59-07:00","relpermalink":"/publication/janus-nsdi19/","section":"publication","summary":"The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability.\nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into the symbolic graph operations. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.","tags":[],"title":"JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs","type":"publication"},{"authors":["Eunji Jeong*","Joo Seong Jeong*","Soojeong Kim","Gyeong-In Yu","Byung-Gon Chun"],"categories":null,"content":"","date":1524515268,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524515268,"objectID":"7b8a8a026cf2e0905d25cd83af7da5a5","permalink":"https://ejjeong.github.io/publication/rdag-eurosys18/","publishdate":"2018-04-23T13:27:48-07:00","relpermalink":"/publication/rdag-eurosys18/","section":"publication","summary":"Recursive neural networks have widely been used by researchers to handle applications with recursively or hierarchically structured data. However, embedded control flow deep learning frameworks such as TensorFlow, Theano, Caffe2, and MXNet fail to efficiently represent and execute such neural networks, due to lack of support for recursion. In this paper, we add recursion to the programming model of existing frameworks by complementing their design with recursive execution of dataflow graphs as well as additional APIs for recursive definitions. Unlike iterative implementations, which can only understand the topological index of each node in recursive data structures, our recursive implementation is able to exploit the recursive relationships between nodes for efficient execution based on parallel computation. We present an implementation on TensorFlow and evaluation results with various recursive neural network models, showing that our recursive implementation not only conveys the recursive nature of recursive neural networks better than other implementations, but also uses given resources more effectively to reduce training and inference time.","tags":[],"title":"Improving the Expressiveness of Deep Learning Frameworks with Recursion","type":"publication"},{"authors":["Soojeong Kim","Eunji Jeong","Joo Seong Jeong","Gyeong-In Yu","Hojin Park","Byung-Gon Chun"],"categories":null,"content":"","date":1509403028,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509403028,"objectID":"ff42003d200afa01146f31b7cea11275","permalink":"https://ejjeong.github.io/publication/parallax-sosp17/","publishdate":"2017-10-30T15:37:08-07:00","relpermalink":"/publication/parallax-sosp17/","section":"publication","summary":"","tags":[],"title":"Auto-Parallelizing Deep Learning for Multi-machine, Multi-GPU Environments","type":"publication"},{"authors":["JinSeok Oh","Sungyu Kim","Eunji Jeong","Soo-Mook Moon"],"categories":null,"content":"","date":1433025231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433025231,"objectID":"3b8e17b186384868dceed23935d0667e","permalink":"https://ejjeong.github.io/publication/coolchips-xviii/","publishdate":"2015-05-30T15:33:51-07:00","relpermalink":"/publication/coolchips-xviii/","section":"publication","summary":"","tags":[],"title":"OS-less Dynamic Binary Instrumentation for Embedded Firmware","type":"publication"},{"authors":["Eunji Jeong"],"categories":null,"content":"","date":1377788400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377788400,"objectID":"8567874ca6b4810035227cb46a9dc722","permalink":"https://ejjeong.github.io/publication/master-thesis/","publishdate":"2013-08-30T00:00:00+09:00","relpermalink":"/publication/master-thesis/","section":"publication","summary":"","tags":[],"title":"(Thesis) Reliable basic block coverage analysis of optimized program using dynamic binary analysis","type":"publication"}]